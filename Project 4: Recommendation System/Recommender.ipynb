{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.021947,
          "end_time": "2022-02-07T14:56:43.864970",
          "exception": false,
          "start_time": "2022-02-07T14:56:43.843023",
          "status": "completed"
        },
        "tags": [],
        "id": "zyntHB33NhFN"
      },
      "source": [
        "# MINI-PROJECT RECSYS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we will build a recommender system (RS) using [*ml-100k* dataset](https://grouplens.org/datasets/movielens/100k/). This data consist on 100,000 ratings from 1000 users on 1700 movies and it is tipically used for either comparing results in SOTA papers or for building toy RS."
      ],
      "metadata": {
        "id": "qSq3oHtsUS4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. You first need to create a folder called *data* and then put inside the downloaded dataset. Thus, your data needs to be stored in:\n",
        "> *./data/ml-100k/all_files_here*.\n",
        "\n",
        "2. Then, in the next cell you can observe how all the model and pipeline configuration is set up. You can modify any parameter if you want to (like the seed, lr, batch_size or embedding dimension...).\n",
        "\n",
        "3. Check also that, in order to allow reproducing the results you achieve, the seed you choose is forwaded to all tensorflow, numpy and os libraries."
      ],
      "metadata": {
        "id": "ZvKeC5b_U3FW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:43.910485Z",
          "iopub.status.busy": "2022-02-07T14:56:43.909603Z",
          "iopub.status.idle": "2022-02-07T14:56:50.477928Z",
          "shell.execute_reply": "2022-02-07T14:56:50.477003Z",
          "shell.execute_reply.started": "2022-02-07T14:53:55.207726Z"
        },
        "papermill": {
          "duration": 6.592,
          "end_time": "2022-02-07T14:56:50.478169",
          "exception": false,
          "start_time": "2022-02-07T14:56:43.886169",
          "status": "completed"
        },
        "tags": [],
        "id": "mtST8SPcNhFO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "DATA_DIR = 'data/ml-100k'\n",
        "OUTPUT_DIR = './'\n",
        "\n",
        "class Config:\n",
        "    category_col = ['user_id','movie_id','Action','Adventure','Animation',\"Childrens\",'Comedy','Crime',\n",
        "          'Documentary','Drama','Fantasy','Film-Noir','Horror','Musical','Mystery',\n",
        "          'Romance','Sci-Fi','Thriller','War','Western', 'gender','occupation','year']\n",
        "    num_col = ['age']\n",
        "    target_col = ['label']\n",
        "    \n",
        "    epochs=5\n",
        "    batch_size=128\n",
        "    seed=17\n",
        "    embedding_dim=8\n",
        "    lr=1e-4\n",
        "    \n",
        "\n",
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "    tf.config.threading.set_intra_op_parallelism_threads(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config=Config()\n",
        "seed_everything(config.seed)"
      ],
      "metadata": {
        "id": "3LRvCg-_Wgwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.01992,
          "end_time": "2022-02-07T14:56:50.520387",
          "exception": false,
          "start_time": "2022-02-07T14:56:50.500467",
          "status": "completed"
        },
        "tags": [],
        "id": "4YllQeedNhFP"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will load the three different files available on ml-100k dataset: \n",
        "\n",
        "- the data interactions\n",
        "- the user data\n",
        "- the item's data"
      ],
      "metadata": {
        "id": "PkSuv3nWW0UF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:50.578774Z",
          "iopub.status.busy": "2022-02-07T14:56:50.578057Z",
          "iopub.status.idle": "2022-02-07T14:56:50.582005Z",
          "shell.execute_reply": "2022-02-07T14:56:50.582585Z",
          "shell.execute_reply.started": "2022-02-07T14:54:00.449957Z"
        },
        "papermill": {
          "duration": 0.041234,
          "end_time": "2022-02-07T14:56:50.582826",
          "exception": false,
          "start_time": "2022-02-07T14:56:50.541592",
          "status": "completed"
        },
        "tags": [],
        "id": "c3_Tk0TqNhFP"
      },
      "outputs": [],
      "source": [
        "def load_data_df():\n",
        "    df = pd.read_csv(os.path.join(DATA_DIR, 'u.data'), sep='\\t', header=None)\n",
        "    df.columns = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
        "    return df\n",
        "\n",
        "def load_item_df():\n",
        "    m_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url',\n",
        "          'unknown','Action','Adventure','Animation',\"Children's\",'Comedy','Crime',\n",
        "          'Documentary','Drama','Fantasy','Film-Noir','Horror','Musical','Mystery',\n",
        "          'Romance','Sci-Fi','Thriller','War','Western',]\n",
        "    item_df = pd.read_csv(os.path.join(DATA_DIR, 'u.item'), sep='|', encoding=\"iso-8859-1\", names=m_cols)\n",
        "    item_df = item_df.rename(columns={\"Children's\":'Childrens'})\n",
        "    return item_df\n",
        "\n",
        "def load_user_df():\n",
        "    u_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
        "    user_df = pd.read_csv(os.path.join(DATA_DIR, 'u.user'), sep='|', encoding=\"iso-8859-1\", names=u_cols)\n",
        "    return user_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:50.637465Z",
          "iopub.status.busy": "2022-02-07T14:56:50.636833Z",
          "iopub.status.idle": "2022-02-07T14:56:50.816944Z",
          "shell.execute_reply": "2022-02-07T14:56:50.816359Z",
          "shell.execute_reply.started": "2022-02-07T14:54:00.460660Z"
        },
        "papermill": {
          "duration": 0.209114,
          "end_time": "2022-02-07T14:56:50.817093",
          "exception": false,
          "start_time": "2022-02-07T14:56:50.607979",
          "status": "completed"
        },
        "tags": [],
        "id": "qksTYIYtNhFP",
        "outputId": "569ed82d-b9e2-4f9d-a5ad-9da1f3defd83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f1823c79c09f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mitem_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_item_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0muser_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_user_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3a6838785d56>\u001b[0m in \u001b[0;36mload_data_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'u.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'movie_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ml-100k/u.data'"
          ]
        }
      ],
      "source": [
        "data_df = load_data_df()\n",
        "item_df = load_item_df()\n",
        "user_df = load_user_df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:50.869379Z",
          "iopub.status.busy": "2022-02-07T14:56:50.868668Z",
          "iopub.status.idle": "2022-02-07T14:56:50.885446Z",
          "shell.execute_reply": "2022-02-07T14:56:50.884897Z",
          "shell.execute_reply.started": "2022-02-07T14:54:00.615207Z"
        },
        "papermill": {
          "duration": 0.046065,
          "end_time": "2022-02-07T14:56:50.885585",
          "exception": false,
          "start_time": "2022-02-07T14:56:50.839520",
          "status": "completed"
        },
        "tags": [],
        "id": "cDmn0UHmNhFP"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "# Excercice 1: \n",
        "#\n",
        "# You can now visualize each of them and get familiar with the data by using dataframe.head() function.\n",
        "#\n",
        "###############################\n",
        "\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> #### Please, check the [pandas.merge documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) for more clarifying information about the next exercice.\n"
      ],
      "metadata": {
        "id": "JQzT2XPSYGt1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:51.111184Z",
          "iopub.status.busy": "2022-02-07T14:56:51.109872Z",
          "iopub.status.idle": "2022-02-07T14:56:51.284257Z",
          "shell.execute_reply": "2022-02-07T14:56:51.283561Z",
          "shell.execute_reply.started": "2022-02-07T14:54:00.676659Z"
        },
        "papermill": {
          "duration": 0.201572,
          "end_time": "2022-02-07T14:56:51.284429",
          "exception": false,
          "start_time": "2022-02-07T14:56:51.082857",
          "status": "completed"
        },
        "tags": [],
        "id": "xJ9Fw1AQNhFQ"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "# Excercice 2: \n",
        "#\n",
        "# Build the following function in order to merge the data into one single dataframe, which needs to be merged by \n",
        "# 'inner' mode. \n",
        "#\n",
        "###############################\n",
        "\n",
        "def merge_df(data_df, item_df, user_df):\n",
        "    # ...\n",
        "    return tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:51.338630Z",
          "iopub.status.busy": "2022-02-07T14:56:51.337606Z",
          "iopub.status.idle": "2022-02-07T14:56:51.428644Z",
          "shell.execute_reply": "2022-02-07T14:56:51.428034Z",
          "shell.execute_reply.started": "2022-02-07T14:54:00.833983Z"
        },
        "papermill": {
          "duration": 0.121052,
          "end_time": "2022-02-07T14:56:51.428814",
          "exception": false,
          "start_time": "2022-02-07T14:56:51.307762",
          "status": "completed"
        },
        "tags": [],
        "id": "tSnXIeM2NhFQ"
      },
      "outputs": [],
      "source": [
        "df = merge_df(data_df, item_df, user_df)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "# Excercice 3: \n",
        "#\n",
        "# Check with one command line that our merged dataframe contains 100,000 rows and 31 features per each row. \n",
        "#\n",
        "###############################\n",
        "\n",
        "# ..."
      ],
      "metadata": {
        "id": "qv_s4rI-YYsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02341,
          "end_time": "2022-02-07T14:56:51.060852",
          "exception": false,
          "start_time": "2022-02-07T14:56:51.037442",
          "status": "completed"
        },
        "tags": [],
        "id": "MsGfHKZBNhFQ"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will pre-process the data in order to apply some common filters (such as a a film having a mínimum of 10 views) and also some data transformations to allow the model process numerical and categorical features."
      ],
      "metadata": {
        "id": "pQxmBLs6Yo8w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:51.482005Z",
          "iopub.status.busy": "2022-02-07T14:56:51.480753Z",
          "iopub.status.idle": "2022-02-07T14:56:52.458991Z",
          "shell.execute_reply": "2022-02-07T14:56:52.458391Z",
          "shell.execute_reply.started": "2022-02-07T14:54:00.927613Z"
        },
        "papermill": {
          "duration": 1.00638,
          "end_time": "2022-02-07T14:56:52.459150",
          "exception": false,
          "start_time": "2022-02-07T14:56:51.452770",
          "status": "completed"
        },
        "tags": [],
        "id": "aDsH9kIANhFQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def build_preprocessor(config): \n",
        "    category_col = config.category_col\n",
        "    num_col = config.num_col\n",
        "    \n",
        "    num_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "        ('std', (StandardScaler())),])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value='NAN')),\n",
        "        ('oe', (OrdinalEncoder())),\n",
        "        ])\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', num_transformer, num_col),\n",
        "            ('cat', categorical_transformer, category_col),\n",
        "        ],\n",
        "        remainder=\"drop\")\n",
        "    return preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "# Excercice 4: \n",
        "#\n",
        "# Explain, by understanding the code of the previous cell and by researching in the documentation what is the\n",
        "# function 'build_preprocessor' doing to our data. Answer in the next cell.\n",
        "#\n",
        "###############################"
      ],
      "metadata": {
        "id": "XboaI5jzY_gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Answer here:**"
      ],
      "metadata": {
        "id": "UWUtPOibZQuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "# Excercice 5: \n",
        "#\n",
        "# Now, by following the next steps we propose, please try to come up with the code that corresponds to the  \n",
        "# filters that need to be done. We have splitted the steps into a), b), c) and d)\n",
        "#\n",
        "###############################"
      ],
      "metadata": {
        "id": "Bd-cbnlKZfxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_col = config.category_col\n",
        "num_col = config.num_col\n",
        "target_col = config.target_col[0]"
      ],
      "metadata": {
        "id": "JtuvjlJ-PPsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######\n",
        "# a) Apply the filter in order to just use movies with more than 10 views\n",
        "#######\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "df = # ...\n",
        "\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "OarpwP_VPRvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######\n",
        "# b) Extract the year from the 'release_date' column.\n",
        "#######\n",
        "\n",
        "df[\"year\"] = # ..."
      ],
      "metadata": {
        "id": "JhJyAaRNPVfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######\n",
        "# c) Create a label column for binary classification from the 'rating' column. If rating >=4, we want the target_col\n",
        "#    to be 1, and if not 0. \n",
        "#######\n",
        "\n",
        "df['label'] = # ...\n",
        "df[target_col]"
      ],
      "metadata": {
        "id": "cLxTA-GSPpb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build pipeline\n",
        "pp = build_preprocessor(config)\n",
        "pp.fit(df)"
      ],
      "metadata": {
        "id": "18Xqn3PiPNUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:54.518502Z",
          "iopub.status.busy": "2022-02-07T14:56:54.517842Z",
          "iopub.status.idle": "2022-02-07T14:56:55.748768Z",
          "shell.execute_reply": "2022-02-07T14:56:55.748141Z",
          "shell.execute_reply.started": "2022-02-07T14:54:02.922735Z"
        },
        "papermill": {
          "duration": 1.263564,
          "end_time": "2022-02-07T14:56:55.748932",
          "exception": false,
          "start_time": "2022-02-07T14:56:54.485368",
          "status": "completed"
        },
        "tags": [],
        "id": "7m3yXMubNhFQ"
      },
      "outputs": [],
      "source": [
        "# Check that it transforms the data (do not do it yet)\n",
        "pp.transform(df).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:55.813637Z",
          "iopub.status.busy": "2022-02-07T14:56:55.812568Z",
          "iopub.status.idle": "2022-02-07T14:56:55.948038Z",
          "shell.execute_reply": "2022-02-07T14:56:55.948660Z",
          "shell.execute_reply.started": "2022-02-07T14:54:03.633171Z"
        },
        "papermill": {
          "duration": 0.173481,
          "end_time": "2022-02-07T14:56:55.948883",
          "exception": false,
          "start_time": "2022-02-07T14:56:55.775402",
          "status": "completed"
        },
        "tags": [],
        "id": "biKMWIdpNhFR"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "# d) Split the data into 80% (train) and 20% (test) with the imported function 'train_test_split'. Look at the \n",
        "#    documentation and use df['movie_id'] as 'stratify' parameter and config.seed as 'random_state'.\n",
        "#######\n",
        "tra_df, val_df = # ...\n",
        "print(tra_df.shape)\n",
        "print(val_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:56.008774Z",
          "iopub.status.busy": "2022-02-07T14:56:56.006579Z",
          "iopub.status.idle": "2022-02-07T14:56:56.036620Z",
          "shell.execute_reply": "2022-02-07T14:56:56.035128Z",
          "shell.execute_reply.started": "2022-02-07T14:54:03.736186Z"
        },
        "papermill": {
          "duration": 0.061755,
          "end_time": "2022-02-07T14:56:56.036866",
          "exception": false,
          "start_time": "2022-02-07T14:56:55.975111",
          "status": "completed"
        },
        "tags": [],
        "id": "Kweq-EjqNhFR"
      },
      "outputs": [],
      "source": [
        "assert tra_df.movie_id.nunique() == val_df.movie_id.nunique()\n",
        "assert len(set(val_df.user_id) - set(tra_df.user_id)) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.028326,
          "end_time": "2022-02-07T14:56:56.096963",
          "exception": false,
          "start_time": "2022-02-07T14:56:56.068637",
          "status": "completed"
        },
        "tags": [],
        "id": "wVzwt0bLNhFR"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will build the model and train it with the data we have been preparing."
      ],
      "metadata": {
        "id": "9ff9K4YwbLb5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:56.161899Z",
          "iopub.status.busy": "2022-02-07T14:56:56.160815Z",
          "iopub.status.idle": "2022-02-07T14:56:56.178967Z",
          "shell.execute_reply": "2022-02-07T14:56:56.179429Z",
          "shell.execute_reply.started": "2022-02-07T14:54:03.756202Z"
        },
        "papermill": {
          "duration": 0.050254,
          "end_time": "2022-02-07T14:56:56.179617",
          "exception": false,
          "start_time": "2022-02-07T14:56:56.129363",
          "status": "completed"
        },
        "tags": [],
        "id": "hNBWVMZzNhFR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, add, Activation, dot\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2 as l2_reg\n",
        "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import itertools\n",
        "\n",
        "\n",
        "def build_model(category_num, category_cols, num_cols, K=8, l2=0.0, l2_fm=0.0):\n",
        "\n",
        "    # Numerical features\n",
        "    num_inputs = [Input(shape=(1,), name=col,) for col in num_cols]\n",
        "    # Categorical features\n",
        "    cat_inputs = [Input(shape=(1,), name=col,) for col in category_cols]\n",
        "\n",
        "    inputs = num_inputs + cat_inputs\n",
        "\n",
        "    flatten_layers=[]\n",
        "    # Numerical featrue embedding\n",
        "    for enc_inp, col in zip(num_inputs, num_cols):\n",
        "        # num feature dense\n",
        "        x = Dense(K, name = f'embed_{col}',kernel_regularizer=l2_reg(l2_fm))(enc_inp)\n",
        "        flatten_layers.append(x)\n",
        "\n",
        "    # Category feature embedding\n",
        "    for enc_inp, col in zip(cat_inputs, category_cols):\n",
        "        num_c = category_num[col]\n",
        "        embed_c = Embedding(input_dim=num_c,\n",
        "                            output_dim=K,\n",
        "                            input_length=1,\n",
        "                            name=f'embed_{col}',\n",
        "                            embeddings_regularizer=l2_reg(l2_fm))(enc_inp)\n",
        "        flatten_c = Flatten()(embed_c)\n",
        "        flatten_layers.append(flatten_c)\n",
        "                \n",
        "    # Feature interaction term\n",
        "    fm_layers = []\n",
        "    for emb1,emb2 in itertools.combinations(flatten_layers, 2):\n",
        "        dot_layer = dot([emb1,emb2], axes=1)\n",
        "        fm_layers.append(dot_layer)        \n",
        "\n",
        "    # Linear term\n",
        "    for enc_inp,col in zip(cat_inputs, category_cols):\n",
        "        # embedding\n",
        "        num_c = category_num[col]\n",
        "        embed_c = Embedding(input_dim=num_c,\n",
        "                            output_dim=1,\n",
        "                            input_length=1,\n",
        "                            name=f'linear_{col}',\n",
        "                            embeddings_regularizer=l2_reg(l2_fm))(enc_inp)\n",
        "        flatten_c = Flatten()(embed_c)\n",
        "        fm_layers.append(flatten_c)\n",
        "                \n",
        "    for enc_inp, col in zip(num_inputs, num_cols):\n",
        "        x = Dense(1, name = f'linear_{col}',kernel_regularizer=l2_reg(l2_fm))(enc_inp)\n",
        "        fm_layers.append(x)\n",
        "\n",
        "    # Add all terms\n",
        "    flatten = add(fm_layers)\n",
        "    outputs = Activation('sigmoid',name='outputs')(flatten)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:56.241358Z",
          "iopub.status.busy": "2022-02-07T14:56:56.240355Z",
          "iopub.status.idle": "2022-02-07T14:56:59.112754Z",
          "shell.execute_reply": "2022-02-07T14:56:59.112050Z",
          "shell.execute_reply.started": "2022-02-07T14:54:03.771595Z"
        },
        "papermill": {
          "duration": 2.906029,
          "end_time": "2022-02-07T14:56:59.112908",
          "exception": false,
          "start_time": "2022-02-07T14:56:56.206879",
          "status": "completed"
        },
        "tags": [],
        "id": "K9rw0SHbNhFR"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "# Excercice 6: \n",
        "#\n",
        "# Make sure you understand the model from above and then call an instance of it and compile it with \n",
        "# optimizer 'adam', loss 'binary_crossentropy' and metrics 'accuracy'.\n",
        "#\n",
        "###############################\n",
        "\n",
        "category_num = {col: df[col].nunique() for col in config.category_col}\n",
        "\n",
        "model = # instance the model here\n",
        "# complile the model here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will build an 'early stopping' callback and also transform the training and validation dataframes with the **pp.transform** function we tested at the beggining of the notebook."
      ],
      "metadata": {
        "id": "FiuENQADemjm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:56:59.230478Z",
          "iopub.status.busy": "2022-02-07T14:56:59.229283Z",
          "iopub.status.idle": "2022-02-07T14:59:11.151139Z",
          "shell.execute_reply": "2022-02-07T14:59:11.150211Z",
          "shell.execute_reply.started": "2022-02-07T14:54:06.161667Z"
        },
        "papermill": {
          "duration": 131.954346,
          "end_time": "2022-02-07T14:59:11.151319",
          "exception": false,
          "start_time": "2022-02-07T14:56:59.196973",
          "status": "completed"
        },
        "tags": [],
        "id": "aO55b03VNhFR"
      },
      "outputs": [],
      "source": [
        "cb = [EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=2, verbose=0,)]\n",
        "\n",
        "feature_num = len(config.category_col + config.num_col)\n",
        "tra_inputs = [pp.transform(tra_df)[:, i] for i in range(feature_num)]\n",
        "val_inputs = [pp.transform(val_df)[:, i] for i in range(feature_num)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "# Excercice 7: \n",
        "#\n",
        "# Complete the fit function with the necessary arguments.\n",
        "#\n",
        "###############################\n",
        "\n",
        "history = model.fit(\n",
        "          x= #... ,\n",
        "          y= #... ,\n",
        "          epochs= #... ,\n",
        "          batch_size= #... ,\n",
        "          validation_data= #(... , ... ),\n",
        "          callbacks= # ...\n",
        "         )"
      ],
      "metadata": {
        "id": "Yd54-c5xRxoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the next cell we give you a function in order to plot the training and validation curves. Plot them and comment the results:\n",
        "\n",
        "> **Answer here**:"
      ],
      "metadata": {
        "id": "q_zLsCdHfR28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:59:11.911964Z",
          "iopub.status.busy": "2022-02-07T14:59:11.911304Z",
          "iopub.status.idle": "2022-02-07T14:59:12.369636Z",
          "shell.execute_reply": "2022-02-07T14:59:12.370782Z",
          "shell.execute_reply.started": "2022-02-07T14:55:47.526765Z"
        },
        "papermill": {
          "duration": 0.839865,
          "end_time": "2022-02-07T14:59:12.370982",
          "exception": false,
          "start_time": "2022-02-07T14:59:11.531117",
          "status": "completed"
        },
        "tags": [],
        "id": "phMvWctqNhFR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "id": "Hn3AXo4GfeyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.367017,
          "end_time": "2022-02-07T14:59:13.089492",
          "exception": false,
          "start_time": "2022-02-07T14:59:12.722475",
          "status": "completed"
        },
        "tags": [],
        "id": "y3jZCV5HNhFR"
      },
      "source": [
        "## Check output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On this last section, what we propose is to select a user and qualitatively check the prediction results to see what our system would recommend for a given user. You can also try with many different users if you want.\n",
        "\n",
        "What we will do is to select the validation results for a given users (those which the model has not seen) and then compute the predictions for each film on this user to compare whether they make sense with the original ratings the user have given (ground-truth)."
      ],
      "metadata": {
        "id": "vO0Df4NXflm_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:59:13.813932Z",
          "iopub.status.busy": "2022-02-07T14:59:13.812445Z",
          "iopub.status.idle": "2022-02-07T14:59:13.849061Z",
          "shell.execute_reply": "2022-02-07T14:59:13.848272Z",
          "shell.execute_reply.started": "2022-02-07T14:55:47.870351Z"
        },
        "papermill": {
          "duration": 0.390776,
          "end_time": "2022-02-07T14:59:13.849225",
          "exception": false,
          "start_time": "2022-02-07T14:59:13.458449",
          "status": "completed"
        },
        "tags": [],
        "id": "aYCOX6dDNhFR"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "# a) Select user_id and select all his/her validation results\n",
        "#####################\n",
        "user = # ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(user_df.index)\n",
        "print(feature_num) # feature_num = len(config.category_col + config.num_col)"
      ],
      "metadata": {
        "id": "VveR1M89TRlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_inputs = [pp.transform(val_df)[user_df.index, i] for i in range(feature_num)]"
      ],
      "metadata": {
        "id": "ZvtNtfbdgOgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:59:15.286524Z",
          "iopub.status.busy": "2022-02-07T14:59:15.285908Z",
          "iopub.status.idle": "2022-02-07T14:59:24.812074Z",
          "shell.execute_reply": "2022-02-07T14:59:24.810594Z",
          "shell.execute_reply.started": "2022-02-07T14:55:47.911157Z"
        },
        "papermill": {
          "duration": 9.884292,
          "end_time": "2022-02-07T14:59:24.812258",
          "exception": false,
          "start_time": "2022-02-07T14:59:14.927966",
          "status": "completed"
        },
        "tags": [],
        "id": "yZ6vXpBYNhFS"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "# b) Predict the results for 'user_inputs' variable and sort their predictions in descending order.\n",
        "#####################\n",
        "\n",
        "user_df['pred'] = # ... compute predictions\n",
        "user_df = # ... sort in descending order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-07T14:59:25.547602Z",
          "iopub.status.busy": "2022-02-07T14:59:25.546613Z",
          "iopub.status.idle": "2022-02-07T14:59:25.562277Z",
          "shell.execute_reply": "2022-02-07T14:59:25.562933Z",
          "shell.execute_reply.started": "2022-02-07T14:55:53.814316Z"
        },
        "papermill": {
          "duration": 0.377057,
          "end_time": "2022-02-07T14:59:25.563114",
          "exception": false,
          "start_time": "2022-02-07T14:59:25.186057",
          "status": "completed"
        },
        "tags": [],
        "id": "MRCCxaWnNhFS"
      },
      "outputs": [],
      "source": [
        "# Finally, here we can observe the rating \n",
        "user_df[['title','rating','pred']].head(50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 171.704459,
      "end_time": "2022-02-07T14:59:28.914960",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-02-07T14:56:37.210501",
      "version": "2.2.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}