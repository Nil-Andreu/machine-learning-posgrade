{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import typing\n",
    "import warnings\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pingouin as pg\n",
    "import matplotlib as mpl\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, HuberRegressor, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.metrics import silhouette_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold, cross_val_score, train_test_split, RandomizedSearchCV\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width: 90% !important}; </style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    message='înternal gelsd'\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=UserWarning\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=RuntimeWarning\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    action='ignore', \n",
    "    category=DataConversionWarning\n",
    ")\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib set label size\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=14)\n",
    "mpl.rc('ytick', labelsize=14)\n",
    "\n",
    "plt.rc('font', size=12)\n",
    "plt.rc('figure', figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47868812",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\n",
    "    'notebook',\n",
    "    font_scale=1,\n",
    "    rc={\n",
    "        'lines.linewidth': 2,\n",
    "        'font.family': [u'times']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24447b",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('./data/train_set.csv', index_col=0)\n",
    "test_set = pd.read_csv('./data/test_set.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2cf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.Price.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['price_range'] = pd.cut(train_set.Price, bins=[.0, .5e6, 1e6, 1.5e6, 2e6, 10e6], labels=[1, 2, 3, 4, 5])\n",
    "train_set['price_range'].hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79eb5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train_set: pd.DataFrame, stratify_col: str = 'price_range'):\n",
    "    train_set_target = train_set[stratify_col]\n",
    "\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(\n",
    "        train_set, \n",
    "        train_set_target, \n",
    "        stratify=train_set_target, \n",
    "        test_size=.15,\n",
    "        random_state=0\n",
    "    )\n",
    "    \n",
    "    return pd.concat([X_train, Y_train], axis=1), pd.concat([X_valid, Y_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e30439",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set = train_val_split(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.drop(['price_range'], axis=1, inplace=True)\n",
    "valid_set.drop(['price_range'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d2b79",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e51d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b59dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Type', data=train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.plot(\n",
    "    kind='scatter',\n",
    "    x='Longtitude',\n",
    "    y='Lattitude',\n",
    "    alpha=.3,\n",
    "    figsize=(20, 10),\n",
    "    c='Price', \n",
    "    cmap=plt.get_cmap('jet'),\n",
    "    colorbar=True,\n",
    "    sharex=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.loc[:, 'Price_log'] = np.log(train_set['Price'])\n",
    "corr = train_set.corr(numeric_only=True)\n",
    "\n",
    "# Getting the Upper Triangle of the co-relation matrix\n",
    "matrix = np.triu(corr)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    annot=True,\n",
    "    mask=matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sorted = corr['Price'].sort_values()\n",
    "corr_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea372d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sorted = corr['Price_log'].sort_values()\n",
    "corr_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02762ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will do pair plot with most correlated values\n",
    "sns.pairplot(\n",
    "    train_set[corr_sorted.index[:3].tolist() + corr_sorted.index[-3:].tolist()],\n",
    "    kind='reg',\n",
    "    plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.1}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f7b5a",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.isnull().sum(axis=0).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddeea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = train_set.copy()\n",
    "valid_set_processed = valid_set.copy()\n",
    "test_set_processed = test_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ff33e",
   "metadata": {},
   "source": [
    "Decided not to handle outliers, but create models that are robust to them. As outliers are a valid data from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd0187",
   "metadata": {},
   "source": [
    "### Handling NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4eb07c",
   "metadata": {},
   "source": [
    "#### Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is the distribution of price when car is not set\n",
    "sns.histplot(\n",
    "    train_set_processed[train_set_processed.Car.isnull()]['Price']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f17918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_null_car(train_df, valid_df, test_df, year_not_car: int = 1940):\n",
    "    \"\"\"\n",
    "    We will assume that for the nulls in the year before year_not_car, are because there as effectively no car.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # For the other ones would place the median\n",
    "    train_set_median = train_df['Car'].median()\n",
    "    \n",
    "    # For very aged houses\n",
    "    train_df.loc[\n",
    "        train_df['YearBuilt'] < year_not_car,\n",
    "        'Car'\n",
    "    ] = 0\n",
    "\n",
    "    valid_df.loc[\n",
    "        valid_df['YearBuilt'] < year_not_car,\n",
    "        'Car'\n",
    "    ] = 0\n",
    "\n",
    "    test_df.loc[\n",
    "        test_df['YearBuilt'] < year_not_car,\n",
    "        'Car'\n",
    "    ] = 0\n",
    "    \n",
    "    \n",
    "    # And for the other houses\n",
    "    train_df['Car'].fillna(train_set_median, inplace=True)\n",
    "    valid_df['Car'].fillna(train_set_median, inplace=True)\n",
    "    test_df['Car'].fillna(train_set_median, inplace=True)\n",
    "    \n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1515b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed, valid_set_processed, test_set_processed = handle_null_car(train_set_processed, valid_set_processed, test_set_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3014e",
   "metadata": {},
   "source": [
    "####  BuildingArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_valid_size(df: pd.DataFrame, column: str):\n",
    "    return df[\n",
    "        (~df[column].isnull()) &\n",
    "        (df[column] > 0)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_invalid_size(df: pd.DataFrame, column: str):\n",
    "    return df[\n",
    "        (df[column].isnull()) |\n",
    "        (df[column] <= 0)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e464a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that half of the dataset is not useful\n",
    "building_area_train_base = get_df_valid_size(train_set_processed, 'BuildingArea')\n",
    "building_area_train_target = get_df_invalid_size(train_set_processed, 'BuildingArea')\n",
    "\n",
    "print('Valid building area shape: ', building_area_train_base.shape)\n",
    "print('Invalid building area shape: ', building_area_train_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa7db7",
   "metadata": {},
   "source": [
    "We can try to find an easy relationship to the building area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61023ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_area_train_base.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8cf047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def create_logs(df: pd.DataFrame, columns: List[str] = ['BuildingArea', 'Rooms', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'YearBuilt']):\n",
    "    for col in columns:\n",
    "        df[col + '_log'] = np.log(df[col] + 1)\n",
    "    \n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we find that there are relationships of log-log of nearly 60&\n",
    "building_area_train_base = create_logs(building_area_train_base)\n",
    "\n",
    "building_area_train_base[\n",
    "    ['BuildingArea', 'BuildingArea_log', \n",
    "    'Rooms', 'Bathroom', 'Bedroom2', 'Car',\n",
    "    'Rooms_log', 'Bedroom2_log', 'Bathroom_log', 'Car_log']\n",
    "].corr()\\\n",
    "    .sort_values('BuildingArea_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83af8b",
   "metadata": {},
   "source": [
    "So for this model, seems the variables that have great influence in the building area log are:\n",
    "- Car\n",
    "- Bathroom Logarithm\n",
    "- Bedroom2 Logarithm\n",
    "- Rooms Logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_area_features = ['Car', 'Bathroom_log', 'Bedroom2_log', 'Rooms_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_building_area_train_base = building_area_train_base.BuildingArea_log\n",
    "X_building_area_train_base = building_area_train_base[building_area_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6803a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_area_scaler = StandardScaler()\n",
    "X_building_area_train_base = building_area_scaler.fit_transform(X_building_area_train_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huber Regressor is robust agains outliers\n",
    "ba_linear_model = HuberRegressor().fit(X_building_area_train_base, Y_building_area_train_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a581a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see in the train set which is r2 scoring\n",
    "from sklearn.metrics import r2_score\n",
    "round(r2_score(Y_building_area_train_base, ba_linear_model.predict(X_building_area_train_base)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570e27e",
   "metadata": {},
   "source": [
    "And now we will use this model to imput the missing values in the building area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def input_nan_logarithms(\n",
    "    model, \n",
    "    scaler, \n",
    "    df: pd.DataFrame, \n",
    "    column_target: str, \n",
    "    column_features,\n",
    "    log: bool = True,\n",
    "    logging: bool = True,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input the nans with a given model with the linear regression model for logarithms.\n",
    "    Also, computes the MSE for the base dataframe (the one without nan).\n",
    "    - model: the model trained\n",
    "    - scaler: StandardScaler fitted\n",
    "    - df: the dataframe for which we want to replace the nans\n",
    "    - column_target: for which column do we want to replace the NaNs\n",
    "    - column_features: list of features used from the df\n",
    "    - log: if we are predicting a logarithm\n",
    "    - logging: if we want to print out scores\n",
    "    \"\"\"\n",
    "    df = deepcopy(df)\n",
    "    column_target_model = column_target + '_log' if log else column_target\n",
    "    \n",
    "    # Get the valid and invalid dataframes\n",
    "    base_df = get_df_valid_size(df, column_target)\n",
    "    target_df = get_df_invalid_size(df, column_target)\n",
    "    \n",
    "    # Create the log features\n",
    "    base_df = create_logs(base_df)\n",
    "    target_df = create_logs(target_df)\n",
    "    \n",
    "    # Apply scaling\n",
    "    Y_base_df = base_df[column_target_model]  # we expect the log of that variable\n",
    "    X_base_df = base_df[column_features]\n",
    "    X_base_df = scaler.transform(X_base_df)\n",
    "    \n",
    "    X_target_df = target_df[column_features]\n",
    "    X_target_df = scaler.transform(X_target_df)\n",
    "    Y_target_df_idx = target_df.index.values\n",
    "\n",
    "    # And now we make the predictions\n",
    "    prediction_base = model.predict(X_base_df)\n",
    "    prediction_target = model.predict(X_target_df)\n",
    "    \n",
    "    # We compute scoring on the base\n",
    "    comparison = [Y_base_df, prediction_base]\n",
    "    comparison = [np.exp(x) if log else x for x in comparison]\n",
    "    \n",
    "    # Compute accuracy on the base\n",
    "    if logging:\n",
    "        print('-'*20)\n",
    "        print('MSE error: ', mean_squared_error(\n",
    "            *comparison,\n",
    "            squared=False\n",
    "        ))\n",
    "\n",
    "        print('R2 error: ', r2_score(\n",
    "            *comparison,\n",
    "        ))\n",
    "        print('-'*20)\n",
    "    \n",
    "    # And finally we fill with the exponential\n",
    "    df.loc[\n",
    "        Y_target_df_idx,\n",
    "        column_target\n",
    "    ] = np.exp(prediction_target)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = input_nan_logarithms(ba_linear_model, building_area_scaler, train_set_processed, 'BuildingArea', building_area_features, logging=False)\n",
    "valid_set_processed = input_nan_logarithms(ba_linear_model, building_area_scaler, valid_set_processed, 'BuildingArea', building_area_features)\n",
    "test_set_processed = input_nan_logarithms(ba_linear_model, building_area_scaler, test_set_processed, 'BuildingArea', building_area_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a3532",
   "metadata": {},
   "source": [
    "#### Landsize\n",
    "\n",
    "This one does not have nulls, but have land size of 0. Which are values we can assumed that were put when they did not have a value for the land size.\n",
    "\n",
    "Same analysis as before, but could include now the Building area & building area logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256314fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that half of the dataset is not useful\n",
    "landsize_train_base = get_df_valid_size(train_set_processed, 'Landsize')\n",
    "landsize_train_target = get_df_invalid_size(train_set_processed, 'Landsize')\n",
    "\n",
    "print('Valid landsize shape: ', landsize_train_base.shape)\n",
    "print('Invalid landsize shape: ', landsize_train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39771bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsize_train_base = create_logs(landsize_train_base)\n",
    "\n",
    "landsize_train_base[\n",
    "    ['Landsize', 'Landsize_log', 'BuildingArea', 'BuildingArea_log', \n",
    "    'Rooms', 'Bathroom', 'Bedroom2', 'Car',\n",
    "    'Rooms_log', 'Bedroom2_log', 'Bathroom_log', 'Car_log']\n",
    "].corr()\\\n",
    "    .sort_values('Landsize_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026b056",
   "metadata": {},
   "source": [
    "Have tried training for both Landsize & Landsize log, and I get better R2 scoring for the Landsize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a073dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsize_features = ['Bathroom', 'BuildingArea', 'Car', 'Rooms', 'Bedroom2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f30fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_landsize_train_base = landsize_train_base.Landsize\n",
    "X_landsize_train_base = landsize_train_base[landsize_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ab2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsize_scaler = StandardScaler()\n",
    "X_landsize_train_base = landsize_scaler.fit_transform(X_landsize_train_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_linear_model = HuberRegressor().fit(X_landsize_train_base, Y_landsize_train_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a930b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(r2_score(Y_landsize_train_base, ls_linear_model.predict(X_landsize_train_base)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b808663",
   "metadata": {},
   "source": [
    "Will try to fill the NaNs with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = input_nan_logarithms(ls_linear_model, landsize_scaler, train_set_processed, 'Landsize', landsize_features, log=False, logging=False)\n",
    "_ = input_nan_logarithms(ls_linear_model, landsize_scaler, valid_set_processed, 'Landsize', landsize_features, log=False)\n",
    "_ = input_nan_logarithms(ls_linear_model, landsize_scaler, test_set_processed, 'Landsize', landsize_features, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see a huge MSE, so we will go for another way to fill the nans\n",
    "pd.DataFrame(train_set_processed\\\n",
    "    .groupby(['Regionname', 'Type'])\\\n",
    "    .median(numeric_only=True)['Landsize'])\\\n",
    "    .T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will handle this grouping\n",
    "train_set_processed['Regionname'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2632ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_region_name_cols = ['Eastern Victoria', 'Western Victoria', 'Northern Victoria']\n",
    "\n",
    "train_set_processed.loc[\n",
    "    train_set_processed['Regionname'].isin(other_region_name_cols),\n",
    "    'Regionname'\n",
    "] = 'Other'\n",
    "\n",
    "valid_set_processed.loc[\n",
    "    valid_set_processed['Regionname'].isin(other_region_name_cols),\n",
    "    'Regionname'\n",
    "] = 'Other'\n",
    "\n",
    "test_set_processed.loc[\n",
    "    test_set_processed['Regionname'].isin(other_region_name_cols),\n",
    "    'Regionname'\n",
    "] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can check again\n",
    "train_set_processed['Regionname'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsize_by_region_type = pd.DataFrame(train_set_processed\\\n",
    "    .groupby(['Regionname', 'Type'])\\\n",
    "    .median(numeric_only=True)['Landsize'])  # median to do not be influenced by outliers of the 0\n",
    "\n",
    "landsize_by_region_type.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this one will be used in the case we get a 0\n",
    "landsize_by_region = pd.DataFrame(train_set_processed\\\n",
    "    .groupby(['Regionname'])\\\n",
    "    .median(numeric_only=True)['Landsize'])\n",
    "\n",
    "landsize_by_region.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa245e",
   "metadata": {},
   "source": [
    "Will use this as a hash map, of values that will be replaced for the NaN of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_null_landsize(df: pd.DataFrame):\n",
    "    for idx, row in df.iterrows():\n",
    "        landsize = row['Landsize']\n",
    "        \n",
    "        if pd.isnull(landsize) or landsize == 0:\n",
    "            region_name = row['Regionname']\n",
    "            type_name = row['Type']\n",
    "            \n",
    "            new_value = landsize_by_region_type.loc[region_name, type_name].values[0]\n",
    "            new_value = new_value if new_value > 0.0 else landsize_by_region.loc[region_name].values[0]\n",
    "            \n",
    "            df.loc[\n",
    "                idx, 'Landsize'\n",
    "            ] = new_value\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10398ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = handle_null_landsize(train_set_processed)\n",
    "valid_set_processed = handle_null_landsize(valid_set_processed)\n",
    "test_set_processed = handle_null_landsize(test_set_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf86e5",
   "metadata": {},
   "source": [
    "#### CouncilArea\n",
    "\n",
    "Will can try to use a **k-nearest neighbor**, as the council area depends a lot in the position of longtitude and lattitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6596ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_processed.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_base = train_set_processed.dropna(subset=['CouncilArea'])\n",
    "train_set_target = train_set_processed[train_set_processed['CouncilArea'].isna()]\n",
    "\n",
    "# Explicative variables\n",
    "X_train_base_set_council = train_set_base[['Lattitude', 'Longtitude']]\n",
    "X_train_target_set_council = train_set_target[['Lattitude', 'Longtitude']]\n",
    "X_train_target_set_council_index = X_train_target_set_council.index\n",
    "\n",
    "# Target variables\n",
    "Y_train_base_set_council = train_set_base['CouncilArea'].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the inputs\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_base_set_council = scaler.fit_transform(X_train_base_set_council)\n",
    "X_train_target_set_council = scaler.transform(X_train_target_set_council)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef4d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "n_council_area = train_set_processed['CouncilArea'].nunique()\n",
    "knn_council_area = KNeighborsClassifier(n_council_area)\n",
    "\n",
    "knn_council_area.fit(X_train_base_set_council, Y_train_base_set_council)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set_base = valid_set_processed.dropna(subset='CouncilArea')\n",
    "X_valid_base_set_council = valid_set_base[['Lattitude', 'Longtitude']]\n",
    "Y_valid_base_set_council = valid_set_base['CouncilArea'].values.ravel()\n",
    "\n",
    "accuracy_score(knn_council_area.predict(X_valid_base_set_council), Y_valid_base_set_council)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef6d3e",
   "metadata": {},
   "source": [
    "We can see that this is not giving us very good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcccc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed['CouncilArea'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5669f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed['CouncilArea'].fillna('Unknown', inplace=True)\n",
    "valid_set_processed['CouncilArea'].fillna('Unknown', inplace=True)\n",
    "test_set_processed['CouncilArea'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a455ad",
   "metadata": {},
   "source": [
    "### YearBuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that half of the dataset is not useful\n",
    "yearbuilt_train_base = get_df_valid_size(train_set_processed, 'YearBuilt')\n",
    "yearbuilt_train_target = get_df_invalid_size(train_set_processed, 'YearBuilt')\n",
    "\n",
    "print('Valid year built shape: ', yearbuilt_train_base.shape)\n",
    "print('Invalid year built shape: ', yearbuilt_train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearbuilt_train_base = create_logs(yearbuilt_train_base)\n",
    "\n",
    "yearbuilt_train_base[\n",
    "    ['YearBuilt', 'YearBuilt_log', 'Landsize', 'Landsize_log', 'BuildingArea', 'BuildingArea_log', \n",
    "    'Rooms', 'Bathroom', 'Bedroom2', 'Car',\n",
    "    'Rooms_log', 'Bedroom2_log', 'Bathroom_log', 'Car_log']\n",
    "].corr()\\\n",
    "    .sort_values('YearBuilt_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353bc21",
   "metadata": {},
   "source": [
    "So we could try out a model with:\n",
    "- Car Logarithm\n",
    "- Car\n",
    "- Bathroom Logarithm\n",
    "- Bathroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd679c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearbuilt_features = ['Car_log', 'Car', 'Bathroom_log', 'Bathroom']\n",
    "Y_yearbuilt_train_base = yearbuilt_train_base.YearBuilt_log\n",
    "X_yearbuilt_train_base = yearbuilt_train_base[yearbuilt_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c1078",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearbuilt_scaler = StandardScaler()\n",
    "X_yearbuilt_train_base = yearbuilt_scaler.fit_transform(X_yearbuilt_train_base)\n",
    "yb_linear_model = HuberRegressor().fit(X_yearbuilt_train_base, Y_yearbuilt_train_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9323bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can see a model that has good \n",
    "round(r2_score(Y_yearbuilt_train_base, yb_linear_model.predict(X_yearbuilt_train_base)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = input_nan_logarithms(yb_linear_model, yearbuilt_scaler, train_set_processed, 'YearBuilt', yearbuilt_features, logging=False)\n",
    "valid_set_processed = input_nan_logarithms(yb_linear_model, yearbuilt_scaler, valid_set_processed, 'YearBuilt', yearbuilt_features)\n",
    "test_set_processed = input_nan_logarithms(yb_linear_model, yearbuilt_scaler, test_set_processed, 'YearBuilt', yearbuilt_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf17a36",
   "metadata": {},
   "source": [
    "With this, we have handled all the Nulls that we had for this df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29068256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set_processed.isnull().sum(axis=0).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d25ef",
   "metadata": {},
   "source": [
    "## Handling Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba13da",
   "metadata": {},
   "source": [
    "There are some categorical variables that have too much different values.\n",
    "\n",
    "So instead of using all of them, we are going to group them depending if they tend to have high/medium/low price values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_grouped_field(\n",
    "    df,\n",
    "    field: str, \n",
    "    expensive_divisor: int = 1.5e6, \n",
    "    premium_divisor: int = 2e6,\n",
    "    font_size: int = 6\n",
    "):\n",
    "    \n",
    "    grouped_pricing = df\\\n",
    "        .groupby(field)\\\n",
    "        .mean()['Price']\\\n",
    "        .sort_values()\n",
    "    \n",
    "    grouped_names = grouped_pricing.index.values.tolist()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=grouped_names,\n",
    "        y=grouped_pricing,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    plt.axhline(\n",
    "        y=expensive_divisor, \n",
    "        color='green',\n",
    "        linestyle='--',\n",
    "        label='Expensive divisor'\n",
    "    )\n",
    "\n",
    "    plt.axhline(\n",
    "        y=premium_divisor, \n",
    "        color='orange',\n",
    "        linestyle='dotted',\n",
    "        label='Premium divisor'\n",
    "    )\n",
    "\n",
    "    plt.xticks(\n",
    "        size=font_size,\n",
    "        rotation=90\n",
    "    );\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "    return grouped_pricing, grouped_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_category(\n",
    "    group_pricing, \n",
    "    df: pd.DataFrame,\n",
    "    input_column_name: str,\n",
    "    output_column_name: str,\n",
    "    premium_threshold = 2e6,\n",
    "    expensive_threshold = 1.5e6,\n",
    "    \n",
    "):\n",
    "    \n",
    "    # We get the group names\n",
    "    premium_group = list(\n",
    "        group_pricing[group_pricing >= premium_threshold].index.values\n",
    "    )\n",
    "    \n",
    "    expensive_group = list(\n",
    "        group_pricing[\n",
    "            (group_pricing >= expensive_threshold) & \n",
    "            (group_pricing < premium_threshold)\n",
    "        ].index.values\n",
    "    )\n",
    "    \n",
    "    normal_group = list(\n",
    "        group_pricing[group_pricing < expensive_threshold].index.values\n",
    "    )\n",
    "    \n",
    "    # And now we make the classification\n",
    "    df.loc[\n",
    "        df[input_column_name].isin(premium_group), \n",
    "        output_column_name\n",
    "    ] = 2\n",
    "\n",
    "    df.loc[\n",
    "        df[input_column_name].isin(expensive_group), \n",
    "        output_column_name\n",
    "    ] = 1\n",
    "\n",
    "    df.loc[\n",
    "        df[input_column_name].isin(normal_group), \n",
    "        output_column_name\n",
    "    ] = 0\n",
    "    \n",
    "    # And in the case some of it has not been classified: 0 as we consider them less than normal\n",
    "    df[output_column_name].fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55260d9f",
   "metadata": {},
   "source": [
    "### SellerG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f038cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_pricing, _ = plot_distribution_grouped_field(train_set_processed, field='SellerG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c783a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = classify_category(\n",
    "    df=train_set_processed,\n",
    "    group_pricing=seller_pricing,\n",
    "    input_column_name='SellerG',\n",
    "    output_column_name='seller_class',\n",
    "    premium_threshold=2e6,\n",
    "    expensive_threshold=1.5e6,\n",
    ")\n",
    "\n",
    "valid_set_processed = classify_category(\n",
    "    df=valid_set_processed,\n",
    "    group_pricing=seller_pricing,\n",
    "    input_column_name='SellerG',\n",
    "    output_column_name='seller_class',\n",
    "    premium_threshold=2e6,\n",
    "    expensive_threshold=1.5e6,\n",
    ")\n",
    "\n",
    "test_set_processed = classify_category(\n",
    "    df=test_set_processed,\n",
    "    group_pricing=seller_pricing,\n",
    "    input_column_name='SellerG',\n",
    "    output_column_name='seller_class',\n",
    "    premium_threshold=2e6,\n",
    "    expensive_threshold=1.5e6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878f848",
   "metadata": {},
   "source": [
    "Moreover, instead of bucketizing we can create a value that ranges from 0-1 that tells you how much that suburb pricing is.\n",
    "\n",
    "TODO: Do not standard scale this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcd47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scoring_mean_price(df: pd.DataFrame, column_grouped: str, price_column: str = 'Price'):\n",
    "    df_means = pd.DataFrame(df\\\n",
    "        .groupby(column_grouped)\\\n",
    "        .mean()[price_column]\\\n",
    "        .sort_values())\n",
    "\n",
    "    max_df_price_grouped = df_means[price_column].max()\n",
    "    df_means['price_mean_proportion'] = df_means[price_column] / max_df_price_grouped\n",
    "    \n",
    "    return df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scoring_median_price(df: pd.DataFrame, column_grouped: str, price_column: str = 'Price'):\n",
    "    df_medians = pd.DataFrame(df\\\n",
    "        .groupby(column_grouped)\\\n",
    "        .median()[price_column]\\\n",
    "        .sort_values())\n",
    "\n",
    "    max_df_price_grouped = df_medians[price_column].max()\n",
    "    df_medians['price_median_proportion'] = df_medians[price_column] / max_df_price_grouped\n",
    "    \n",
    "    return df_medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_mean = get_scoring_mean_price(train_set_processed, 'SellerG')\n",
    "seller_median = get_scoring_median_price(train_set_processed, 'SellerG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc8d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_mean.price_mean_proportion.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_median.price_median_proportion.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_price_proportions(\n",
    "    df: pd.DataFrame, \n",
    "    df_price_mean: pd.DataFrame,\n",
    "    target_column_df: str,\n",
    "    suffix: str = '_price_mean_prop'\n",
    "):\n",
    "    # By default will be values of 0, in the case we have not seem some\n",
    "    new_column = str(target_column_df + suffix).lower()\n",
    "    df.loc[:, new_column] = .0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            df.loc[idx, new_column] = df_price_mean.loc[row[target_column_df]].values[1]\n",
    "        \n",
    "        # In case some seller is not found\n",
    "        except:\n",
    "            df.loc[idx, new_column] = 0\n",
    "            continue \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we place those values into the dataframe\n",
    "train_set_processed = set_price_proportions(train_set_processed, seller_mean, 'SellerG')\n",
    "valid_set_processed = set_price_proportions(valid_set_processed, seller_mean, 'SellerG')\n",
    "test_set_processed = set_price_proportions(test_set_processed, seller_mean, 'SellerG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcee60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = set_price_proportions(train_set_processed, seller_median, 'SellerG', '_price_median_prop')\n",
    "valid_set_processed = set_price_proportions(valid_set_processed, seller_median, 'SellerG', '_price_median_prop')\n",
    "test_set_processed = set_price_proportions(test_set_processed, seller_median, 'SellerG', '_price_median_prop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66aa04",
   "metadata": {},
   "source": [
    "### Suburb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_pricing, _ = plot_distribution_grouped_field(\n",
    "    train_set_processed,\n",
    "    field='Suburb', \n",
    "    expensive_divisor=1.5e6, \n",
    "    premium_divisor=1.9e6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55597ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = classify_category(\n",
    "    df=train_set_processed,\n",
    "    group_pricing=suburb_pricing,\n",
    "    input_column_name='Suburb',\n",
    "    output_column_name='suburb_class',\n",
    "    premium_threshold=1.9e6,\n",
    "    expensive_threshold=1.5e6,\n",
    ")\n",
    "\n",
    "valid_set_processed = classify_category(\n",
    "    df=valid_set_processed,\n",
    "    group_pricing=suburb_pricing,\n",
    "    input_column_name='Suburb',\n",
    "    output_column_name='suburb_class',\n",
    "    premium_threshold=1.9e6,\n",
    "    expensive_threshold=1.5e6,\n",
    ")\n",
    "\n",
    "test_set_processed = classify_category(\n",
    "    df=test_set_processed,\n",
    "    group_pricing=suburb_pricing,\n",
    "    input_column_name='Suburb',\n",
    "    output_column_name='suburb_class',\n",
    "    premium_threshold=1.9e6,\n",
    "    expensive_threshold=1.5e6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523935ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_mean = get_scoring_mean_price(train_set_processed, 'Suburb')\n",
    "suburb_median = get_scoring_median_price(train_set_processed, 'Suburb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f37ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_mean.price_mean_proportion.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51116471",
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_median.price_median_proportion.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141bcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = set_price_proportions(train_set_processed, suburb_mean, 'Suburb')\n",
    "valid_set_processed = set_price_proportions(valid_set_processed, suburb_mean, 'Suburb')\n",
    "test_set_processed = set_price_proportions(test_set_processed, suburb_mean, 'Suburb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = set_price_proportions(train_set_processed, suburb_median, 'Suburb', '_price_median_prop')\n",
    "valid_set_processed = set_price_proportions(valid_set_processed, suburb_median, 'Suburb', '_price_median_prop')\n",
    "test_set_processed = set_price_proportions(test_set_processed, suburb_median, 'Suburb', '_price_median_prop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55243c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And remove the SellerG that will no longer be used\n",
    "# train_set_processed.drop(['Suburb'], axis=1, inplace=True)\n",
    "# valid_set_processed.drop(['Suburb'], axis=1, inplace=True)\n",
    "# test_set_processed.drop(['Suburb'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24498d5",
   "metadata": {},
   "source": [
    "### Council Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3409810",
   "metadata": {},
   "outputs": [],
   "source": [
    "council_pricing, _ = plot_distribution_grouped_field(\n",
    "    train_set_processed, \n",
    "    'CouncilArea', \n",
    "    expensive_divisor=1.1e6, \n",
    "    premium_divisor=1.4e6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = classify_category(\n",
    "    df=train_set_processed,\n",
    "    group_pricing=council_pricing,\n",
    "    input_column_name='CouncilArea',\n",
    "    output_column_name='council_class',\n",
    "    premium_threshold=1e6,\n",
    "    expensive_threshold=1.3e6,\n",
    ")\n",
    "\n",
    "valid_set_processed = classify_category(\n",
    "    df=valid_set_processed,\n",
    "    group_pricing=council_pricing,\n",
    "    input_column_name='CouncilArea',\n",
    "    output_column_name='council_class',\n",
    "    premium_threshold=1e6,\n",
    "    expensive_threshold=1.3e6,\n",
    ")\n",
    "\n",
    "test_set_processed = classify_category(\n",
    "    df=test_set_processed,\n",
    "    group_pricing=council_pricing,\n",
    "    input_column_name='CouncilArea',\n",
    "    output_column_name='council_class',\n",
    "    premium_threshold=1e6,\n",
    "    expensive_threshold=1.3e6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "council_mean = get_scoring_mean_price(train_set_processed, 'CouncilArea')\n",
    "council_median = get_scoring_median_price(train_set_processed, 'CouncilArea')\n",
    "council_mean.price_mean_proportion.hist(bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = set_price_proportions(train_set_processed, council_mean, 'CouncilArea')\n",
    "valid_set_processed = set_price_proportions(valid_set_processed, council_mean, 'CouncilArea')\n",
    "test_set_processed = set_price_proportions(test_set_processed, council_mean, 'CouncilArea')\n",
    "\n",
    "train_set_processed = set_price_proportions(train_set_processed, council_median, 'CouncilArea', '_price_median_prop')\n",
    "valid_set_processed = set_price_proportions(valid_set_processed, council_median, 'CouncilArea', '_price_median_prop')\n",
    "test_set_processed = set_price_proportions(test_set_processed, council_median, 'CouncilArea', '_price_median_prop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd31b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_processed.drop(['CouncilArea'], axis=1, inplace=True)\n",
    "# valid_set_processed.drop(['CouncilArea'], axis=1, inplace=True)\n",
    "# test_set_processed.drop(['CouncilArea'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e036ad",
   "metadata": {},
   "source": [
    "### Address\n",
    "\n",
    "There are a lof of different addresses.\n",
    "We could check for the case of some has been re-sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10662bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_resold(\n",
    "    data_duplicated: pd.DataFrame, \n",
    "    data_source: pd.DataFrame\n",
    "):\n",
    "    \n",
    "    before, actual = '', ''\n",
    "    for idx, row in data_duplicated.iterrows():\n",
    "        if not before:\n",
    "            before = row['Address']\n",
    "            data_source.loc[idx, 'Resold'] = 0\n",
    "            continue\n",
    "\n",
    "        actual = row['Address']\n",
    "        if before == actual:\n",
    "            # Set it directly this new feature on extended dataframe\n",
    "            data_source.loc[idx, 'Resold'] = 1\n",
    "\n",
    "        else:\n",
    "            data_source.loc[idx, 'Resold'] = 0\n",
    "            before = actual\n",
    "    \n",
    "    return data_source\n",
    "\n",
    "def check_resold_houses(data: pd.DataFrame):\n",
    "    # Will assume that the ones with same values in address, room, bedroom and bathroom is the same house being sold\n",
    "    multiple_sold = data[\n",
    "        data.duplicated(\n",
    "            subset=['Address', 'Rooms', 'Bedroom2', 'Bathroom'], \n",
    "            keep=False\n",
    "        )\n",
    "    ].sort_values(['Address', 'Date'])\n",
    "    \n",
    "    # And now we add the feature of being sold\n",
    "    if len(multiple_sold) > 0:\n",
    "        data = add_feature_resold(multiple_sold, data)\n",
    "        data['Resold'] = data['Resold'].fillna(0).astype('int8')\n",
    "    else:\n",
    "        data['Resold'] = 0\n",
    "    \n",
    "    # An example with the ones sold more than once\n",
    "    display(data.loc[multiple_sold.index].head(2))\n",
    "    \n",
    "    address_feature = ['Resold']\n",
    "    \n",
    "    return data, address_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab57d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed, _ = check_resold_houses(train_set_processed)\n",
    "valid_set_processed, _ = check_resold_houses(valid_set_processed)\n",
    "test_set_processed, _ = check_resold_houses(test_set_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed.drop(['Address'], axis=1, inplace=True)\n",
    "valid_set_processed.drop(['Address'], axis=1, inplace=True)\n",
    "test_set_processed.drop(['Address'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f2327",
   "metadata": {},
   "source": [
    "### Type, Regionname & Method\n",
    "\n",
    "Simply creating one-hot encoding for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = pd.get_dummies(\n",
    "    train_set_processed, \n",
    "    columns=['Method'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "valid_set_processed = pd.get_dummies(\n",
    "    valid_set_processed, \n",
    "    columns=['Method'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "test_set_processed = pd.get_dummies(\n",
    "    test_set_processed, \n",
    "    columns=['Method'], \n",
    "    drop_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6426685",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = pd.get_dummies(\n",
    "    train_set_processed, \n",
    "    columns=['Type'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "valid_set_processed = pd.get_dummies(\n",
    "    valid_set_processed, \n",
    "    columns=['Type'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "test_set_processed = pd.get_dummies(\n",
    "    test_set_processed, \n",
    "    columns=['Type'], \n",
    "    drop_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = pd.get_dummies(\n",
    "    train_set_processed, \n",
    "    columns=['Regionname'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "valid_set_processed = pd.get_dummies(\n",
    "    valid_set_processed, \n",
    "    columns=['Regionname'], \n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "test_set_processed = pd.get_dummies(\n",
    "    test_set_processed, \n",
    "    columns=['Regionname'], \n",
    "    drop_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a7edc",
   "metadata": {},
   "source": [
    "### Postcode & Date\n",
    "\n",
    "Postcode is a categorical variable, as the numbers does not have an order. So we will remove that one.\n",
    "\n",
    "For date, we will extract the date in which it was sold and the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e74cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping postcodes\n",
    "train_set_processed.drop(['Postcode'], axis=1, inplace=True)\n",
    "valid_set_processed.drop(['Postcode'], axis=1, inplace=True)\n",
    "test_set_processed.drop(['Postcode'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5aa709",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed['Date'] = pd.to_datetime(train_set_processed['Date'])\n",
    "valid_set_processed['Date'] = pd.to_datetime(valid_set_processed['Date'])\n",
    "test_set_processed['Date'] = pd.to_datetime(test_set_processed['Date'])\n",
    "\n",
    "train_set_processed['year_sold'] = train_set_processed['Date'].dt.year\n",
    "valid_set_processed['year_sold'] = valid_set_processed['Date'].dt.year\n",
    "test_set_processed['year_sold'] = test_set_processed['Date'].dt.year\n",
    "\n",
    "train_set_processed['quarter_sold'] = train_set_processed.Date.dt.quarter\n",
    "valid_set_processed['quarter_sold'] = valid_set_processed.Date.dt.quarter\n",
    "test_set_processed['quarter_sold'] = test_set_processed.Date.dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed['years_to_sell'] = train_set_processed['year_sold'] - train_set_processed['YearBuilt']\n",
    "valid_set_processed['years_to_sell'] = valid_set_processed['year_sold'] - valid_set_processed['YearBuilt']\n",
    "test_set_processed['years_to_sell'] = test_set_processed['year_sold'] - test_set_processed['YearBuilt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380451f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping date\n",
    "train_set_processed.drop(['Date'], axis=1, inplace=True)\n",
    "valid_set_processed.drop(['Date'], axis=1, inplace=True)\n",
    "test_set_processed.drop(['Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b23b6f5",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Apart from the features we already created, we are going to add some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab712470",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_extended = train_set_processed.copy()\n",
    "valid_set_extended = valid_set_processed.copy()\n",
    "test_set_extended = test_set_processed.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f600e",
   "metadata": {},
   "source": [
    "### Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22216275",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe82333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding ratios\n",
    "train_set_extended['bed_bath_ratio'] = train_set_extended['Bedroom2'] + 1 / train_set_extended['Bathroom'] + 1\n",
    "train_set_extended['car_bed_ratio'] = train_set_extended['Car'] + 1/ train_set_extended['Bedroom2'] + 1\n",
    "train_set_extended['bed_room_ratio'] = train_set_extended['Bedroom2'] + 1 / train_set_extended['Rooms'] + 1\n",
    "train_set_extended['bath_room_ratio'] = train_set_extended['Bathroom'] + 1 / train_set_extended['Rooms'] + 1\n",
    "train_set_extended['room_building_area_ratio'] = train_set_extended['Rooms'] + 1 / train_set_extended['BuildingArea'] + 1\n",
    "train_set_extended['bed_building_area_ratio'] = train_set_extended['Bedroom2'] + 1 / train_set_extended['BuildingArea'] + 1\n",
    "train_set_extended['bath_building_area_ratio'] = train_set_extended['Bathroom'] + 1 / train_set_extended['BuildingArea'] + 1\n",
    "\n",
    "\n",
    "valid_set_extended['bed_bath_ratio'] = valid_set_extended['Bedroom2'] + 1 / valid_set_extended['Bathroom'] + 1\n",
    "valid_set_extended['car_bed_ratio'] = valid_set_extended['Car'] + 1/ valid_set_extended['Bedroom2'] + 1\n",
    "valid_set_extended['bed_room_ratio'] = valid_set_extended['Bedroom2'] + 1 / valid_set_extended['Rooms'] + 1\n",
    "valid_set_extended['bath_room_ratio'] = valid_set_extended['Bathroom'] + 1 / valid_set_extended['Rooms'] + 1\n",
    "valid_set_extended['room_building_area_ratio'] = valid_set_extended['Rooms'] + 1 / valid_set_extended['BuildingArea'] + 1\n",
    "valid_set_extended['bed_building_area_ratio'] = valid_set_extended['Bedroom2'] + 1 / valid_set_extended['BuildingArea'] + 1\n",
    "valid_set_extended['bath_building_area_ratio'] = valid_set_extended['Bathroom'] + 1 / valid_set_extended['BuildingArea'] + 1\n",
    "\n",
    "\n",
    "test_set_extended['bed_bath_ratio'] = test_set_extended['Bedroom2'] + 1 / test_set_extended['Bathroom'] + 1\n",
    "test_set_extended['car_bed_ratio'] = test_set_extended['Car'] + 1/ test_set_extended['Bedroom2'] + 1\n",
    "test_set_extended['bed_room_ratio'] = test_set_extended['Bedroom2'] + 1 / test_set_extended['Rooms'] + 1\n",
    "test_set_extended['bath_room_ratio'] = test_set_extended['Bathroom'] + 1 / test_set_extended['Rooms'] + 1\n",
    "test_set_extended['room_building_area_ratio'] = test_set_extended['Rooms'] + 1 / test_set_extended['BuildingArea'] + 1\n",
    "test_set_extended['bed_building_area_ratio'] = test_set_extended['Bedroom2'] + 1 / test_set_extended['BuildingArea'] + 1\n",
    "test_set_extended['bath_building_area_ratio'] = test_set_extended['Bathroom'] + 1 / test_set_extended['BuildingArea'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f130c",
   "metadata": {},
   "source": [
    "### Logs & Sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334447dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the logarithms\n",
    "train_set_extended = create_logs(train_set_extended)\n",
    "valid_set_extended = create_logs(valid_set_extended)\n",
    "test_set_extended = create_logs(test_set_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_extended['Price_log'] = np.log(train_set_extended.Price + 1)\n",
    "valid_set_extended['Price_log'] = np.log(valid_set_extended.Price + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd876b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_means_log = get_scoring_mean_price(train_set_extended, 'Suburb', 'Price_log')\n",
    "suburb_median_log = get_scoring_median_price(train_set_extended, 'Suburb', 'Price_log')\n",
    "\n",
    "train_set_extended = set_price_proportions(train_set_extended, suburb_means_log, 'Suburb', '_price_mean_log_prop')\n",
    "train_set_extended = set_price_proportions(train_set_extended, suburb_median_log, 'Suburb', '_price_median_log_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, suburb_means_log, 'Suburb', '_price_mean_log_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, suburb_median_log, 'Suburb', '_price_median_log_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, suburb_means_log, 'Suburb', '_price_mean_log_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, suburb_median_log, 'Suburb', '_price_median_log_prop')\n",
    "\n",
    "council_area_means_log = get_scoring_mean_price(train_set_extended, 'CouncilArea', 'Price_log')\n",
    "council_area_median_log = get_scoring_median_price(train_set_extended, 'CouncilArea', 'Price_log')\n",
    "\n",
    "train_set_extended = set_price_proportions(train_set_extended, council_area_means_log, 'CouncilArea', '_price_mean_log_prop')\n",
    "train_set_extended = set_price_proportions(train_set_extended, council_area_median_log, 'CouncilArea', '_price_median_log_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, council_area_means_log, 'CouncilArea', '_price_mean_log_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, council_area_median_log, 'CouncilArea', '_price_median_log_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, council_area_means_log, 'CouncilArea', '_price_mean_log_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, council_area_median_log, 'CouncilArea', '_price_median_log_prop')\n",
    "\n",
    "seller_means_log = get_scoring_mean_price(train_set_processed, 'SellerG', 'Price_log')\n",
    "seller_median_log = get_scoring_median_price(train_set_processed, 'SellerG', 'Price_log')\n",
    "\n",
    "train_set_extended = set_price_proportions(train_set_extended, seller_means_log, 'SellerG', '_price_mean_log_prop')\n",
    "train_set_extended = set_price_proportions(train_set_extended, seller_median_log, 'SellerG', '_price_median_log_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, seller_means_log, 'SellerG', '_price_mean_log_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, seller_median_log, 'SellerG', '_price_median_log_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, seller_means_log, 'SellerG', '_price_mean_log_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, seller_median_log, 'SellerG', '_price_median_log_prop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e109e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_extended = set_price_proportions(test_set_extended, suburb_means_log, 'Suburb', '_price_mean_log_prop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdb600",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_extended.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And remove the SellerG that will no longer be used\n",
    "to_drop = ['SellerG', 'Suburb', 'CouncilArea']\n",
    "\n",
    "for col in to_drop:\n",
    "    train_set_extended.drop([col], axis=1, inplace=True)\n",
    "    valid_set_extended.drop([col], axis=1, inplace=True)\n",
    "    test_set_extended.drop([col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_extended['Distance_sqr'] = np.sqrt(train_set_extended['Distance'])\n",
    "train_set_extended['Landsize_sqr'] = np.sqrt(train_set_extended['Landsize'])\n",
    "train_set_extended['BuildingArea_sqr'] = np.sqrt(train_set_extended['BuildingArea'])\n",
    "train_set_extended['Propertycount_sqr'] = np.sqrt(train_set_extended['Propertycount'])\n",
    "\n",
    "valid_set_extended['Distance_sqr'] = np.sqrt(valid_set_extended['Distance'])\n",
    "valid_set_extended['Landsize_sqr'] = np.sqrt(valid_set_extended['Landsize'])\n",
    "valid_set_extended['BuildingArea_sqr'] = np.sqrt(valid_set_extended['BuildingArea'])\n",
    "valid_set_extended['Propertycount_sqr'] = np.sqrt(valid_set_extended['Propertycount'])\n",
    "\n",
    "test_set_extended['Distance_sqr'] = np.sqrt(test_set_extended['Distance'])\n",
    "test_set_extended['Landsize_sqr'] = np.sqrt(test_set_extended['Landsize'])\n",
    "test_set_extended['BuildingArea_sqr'] = np.sqrt(test_set_extended['BuildingArea'])\n",
    "test_set_extended['Propertycount_sqr'] = np.sqrt(test_set_extended['Propertycount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adca17f",
   "metadata": {},
   "source": [
    "### Property Count\n",
    "\n",
    "Will bucketize also this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buckets of property count\n",
    "_, pc_bins = pd.qcut(train_set_extended['Propertycount'], 9, labels=False, retbins=True)\n",
    "\n",
    "train_set_extended['prop_count_bkt'] = pd.cut(\n",
    "    train_set_extended['Propertycount'], \n",
    "    bins=pc_bins, \n",
    "    labels=range(len(pc_bins)-1),\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "valid_set_extended['prop_count_bkt'] = pd.cut(\n",
    "    valid_set_extended['Propertycount'], \n",
    "    bins=pc_bins, \n",
    "    labels=range(len(pc_bins)-1),\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "test_set_extended['prop_count_bkt'] = pd.cut(\n",
    "    test_set_extended['Propertycount'], \n",
    "    bins=pc_bins, \n",
    "    labels=range(len(pc_bins)-1),\n",
    "    include_lowest=True\n",
    ")\n",
    "test_set_extended['prop_count_bkt'].fillna(0.0, inplace=True) # one of 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7583e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_extended['prop_count_bkt'] = train_set_extended['prop_count_bkt'].astype('int8')\n",
    "valid_set_extended['prop_count_bkt'] = valid_set_extended['prop_count_bkt'].astype('int8')\n",
    "test_set_extended['prop_count_bkt'] = test_set_extended['prop_count_bkt'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could be a good ratio the price / property count, as might be the ones with more allocation the cheaper\n",
    "prop_count_bkt_mean = get_scoring_mean_price(train_set_extended, 'prop_count_bkt')\n",
    "prop_count_bkt_median = get_scoring_median_price(train_set_extended, 'prop_count_bkt')\n",
    "\n",
    "prop_count_bkt_mean_log = get_scoring_mean_price(train_set_extended, 'prop_count_bkt', 'Price_log')\n",
    "prop_count_bkt_median_log = get_scoring_median_price(train_set_extended, 'prop_count_bkt', 'Price_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_extended = set_price_proportions(train_set_extended, prop_count_bkt_mean, 'prop_count_bkt', '_price_mean_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, prop_count_bkt_mean, 'prop_count_bkt', '_price_mean_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, prop_count_bkt_mean, 'prop_count_bkt', '_price_mean_prop')\n",
    "\n",
    "train_set_extended = set_price_proportions(train_set_extended, prop_count_bkt_median, 'prop_count_bkt', '_price_median_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, prop_count_bkt_median, 'prop_count_bkt', '_price_median_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, prop_count_bkt_median, 'prop_count_bkt', '_price_median_prop')\n",
    "\n",
    "train_set_extended = set_price_proportions(train_set_extended, prop_count_bkt_mean_log, 'prop_count_bkt', '_price_mean_log_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, prop_count_bkt_mean_log, 'prop_count_bkt', '_price_mean_log_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, prop_count_bkt_mean_log, 'prop_count_bkt', '_price_mean_log_prop')\n",
    "\n",
    "train_set_extended = set_price_proportions(train_set_extended, prop_count_bkt_median_log, 'prop_count_bkt', '_price_median_log_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, prop_count_bkt_median_log, 'prop_count_bkt', '_price_median_log_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, prop_count_bkt_median_log, 'prop_count_bkt', '_price_median_log_prop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e4433c",
   "metadata": {},
   "source": [
    "### Longtitude & Lattitude\n",
    "\n",
    "Will assume that there are groups for that location that are of relevent importance for the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_long_lat = train_set_extended[['Longtitude', 'Lattitude']]\n",
    "valid_long_lat = valid_set_extended[['Longtitude', 'Lattitude']]\n",
    "test_long_lat = test_set_extended[['Longtitude', 'Lattitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f73d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = {}\n",
    "for k in range(1, 20):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k, \n",
    "        max_iter=1000, \n",
    "        n_init='auto'\n",
    "    ).fit(train_long_lat)\n",
    "        \n",
    "    # Squarred Sum of Errors\n",
    "    sse[k] = kmeans.inertia_\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c781ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisor = 12\n",
    "n_features = math.floor((train_set['YearBuilt'].nunique() / divisor))\n",
    "\n",
    "print('Number of features: ', n_features)\n",
    "k_means = KMeans(\n",
    "    n_clusters=n_features, \n",
    "    n_init='auto',\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "k_means.fit(train_long_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the same model trained we predict both groups for test & trian\n",
    "labels_train = k_means.labels_\n",
    "labels_valid = k_means.predict(valid_long_lat)\n",
    "labels_test = k_means.predict(test_long_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroides = k_means.cluster_centers_\n",
    "etiquetas = k_means.labels_\n",
    "\n",
    "unique_labels = np.unique(labels_train)\n",
    " \n",
    "for label in unique_labels:\n",
    "    plt.scatter(\n",
    "        train_long_lat.iloc[labels_train == label, 0], \n",
    "        train_long_lat.iloc[labels_train == label, 1], \n",
    "        label = f\"{label} cluster\",\n",
    "    )\n",
    "\n",
    "plt.scatter(\n",
    "    centroides[:,0],\n",
    "    centroides[:,1], \n",
    "    label='centroides', \n",
    "    color = 'k', \n",
    "    s=10\n",
    ")\n",
    "\n",
    "plt.legend(\n",
    "    prop={'size': 8}\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this new feature into the dataframe\n",
    "train_set_extended['location_group'] = labels_train\n",
    "valid_set_extended['location_group'] = labels_valid\n",
    "test_set_extended['location_group'] = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_extended['location_group'] = train_set_extended['location_group'].astype('category')\n",
    "# valid_set_extended['location_group'] = valid_set_extended['location_group'].astype('category')\n",
    "# test_set_extended['location_group'] = test_set_extended['location_group'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce663825",
   "metadata": {},
   "source": [
    "Having those clusters identified can be interesting for then to see distribution of the prices in each one. And letting a model to have a value for which to pivot -/+ with respect to make the prediction.\n",
    "\n",
    "Will also create another cluster for the ones that have price in the 95th percentile or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 9th cluster  -> for prediction of prices\n",
    "percentile_price_train = train_set_extended.Price.quantile(.999)\n",
    "round(percentile_price_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_set_extended.Price).boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a885b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_set_extended.loc[\n",
    "    (train_set_extended.Price > percentile_price_train).values,\n",
    "    'location_group'\n",
    "] = 9\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3220e81",
   "metadata": {},
   "source": [
    "For now, for each of the known clusters we will impute which is the mean/median in each of their locations, and divide by the max in each of their respective locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_group_info = train_set_extended\\\n",
    "    .groupby('location_group')[['Price', 'Price_log']]\\\n",
    "    .describe().T\n",
    "\n",
    "location_group_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_means = get_scoring_mean_price(train_set_extended, 'location_group')\n",
    "location_median = get_scoring_median_price(train_set_extended, 'location_group')\n",
    "\n",
    "location_means_log = get_scoring_mean_price(train_set_extended, 'location_group', 'Price_log')\n",
    "location_median_log = get_scoring_median_price(train_set_extended, 'location_group', 'Price_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a70240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_extended = set_price_proportions(train_set_extended, location_means, 'location_group', '_price_mean_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, location_means, 'location_group', '_price_mean_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, location_means, 'location_group', '_price_mean_prop')\n",
    "\n",
    "train_set_extended = set_price_proportions(train_set_extended, location_median, 'location_group', '_price_median_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, location_median, 'location_group', '_price_median_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, location_median, 'location_group', '_price_median_prop')\n",
    "\n",
    "train_set_extended = set_price_proportions(train_set_extended, location_means_log, 'location_group', '_price_mean_log_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, location_means_log, 'location_group', '_price_mean_log_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, location_means_log, 'location_group', '_price_mean_log_prop')\n",
    "\n",
    "train_set_extended = set_price_proportions(train_set_extended, location_median_log, 'location_group', '_price_median_log_prop')\n",
    "valid_set_extended = set_price_proportions(valid_set_extended, location_median_log, 'location_group', '_price_median_log_prop')\n",
    "test_set_extended = set_price_proportions(test_set_extended, location_median_log, 'location_group', '_price_median_log_prop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb31df",
   "metadata": {},
   "source": [
    "### Sum Up of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set_extended.dtypes.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_extended.isin([-np.inf, np.inf]).sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75631bf",
   "metadata": {},
   "source": [
    "# Model Creation: Price Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_final = train_set_extended.copy()\n",
    "valid_set_final = valid_set_extended.copy()\n",
    "test_set_final = test_set_extended.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f990583",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "corr_final = train_set_final.corr().sort_values('Price')[['Price', 'Price_log']]\n",
    "corr_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More like a normal distribution\n",
    "train_set_final.Price_log.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cc1a3",
   "metadata": {},
   "source": [
    "So we can see it could be easier to predict which is the logarithm of the price, and then afterwards we make the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dbd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features with corr > 10\n",
    "final_features = corr_final[(corr_final.Price_log > .1) | (corr_final.Price_log < -.1)].index.values\n",
    "final_features = [feature for feature in final_features if feature not in ['Price', 'Price_log']]\n",
    "final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will add categorical that were not considered before\n",
    "final_features += ['prop_count_bkt', 'location_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e42ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will standarize those features\n",
    "final_scaler = StandardScaler()\n",
    "\n",
    "Y_train_set_final = train_set_final.Price_log.ravel().reshape(-1, 1)\n",
    "X_train_set_final = train_set_final[final_features]\n",
    "\n",
    "Y_valid_set_final = valid_set_final.Price_log.ravel().reshape(-1, 1)\n",
    "X_valid_set_final = valid_set_final[final_features]\n",
    "\n",
    "X_test_set_final = test_set_final[final_features]\n",
    "\n",
    "X_train_set_final = final_scaler.fit_transform(X_train_set_final)\n",
    "X_valid_set_final = final_scaler.transform(X_valid_set_final)\n",
    "X_test_set_final = final_scaler.transform(X_test_set_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0befe874",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87662979",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd585a4",
   "metadata": {},
   "source": [
    "As requested in the problem, use LR or KNNeighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdee136",
   "metadata": {},
   "source": [
    "### Linear Regressor\n",
    "\n",
    "Can try different linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final_model = LinearRegression().fit(X_train_set_final, Y_train_set_final)\n",
    "\n",
    "lr_pred = np.exp(lr_final_model.predict(X_valid_set_final))\n",
    "print('Mean Squarred Error', mean_squared_error(lr_pred, np.exp(Y_valid_set_final), squared=False))\n",
    "print('Mean Absolute Error', mean_absolute_error(lr_pred, np.exp(Y_valid_set_final)))\n",
    "print('R2 Score', r2_score(lr_pred, np.exp(Y_valid_set_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final_model = HuberRegressor().fit(X_train_set_final, Y_train_set_final)\n",
    "\n",
    "lr_pred = np.exp(lr_final_model.predict(X_valid_set_final))\n",
    "print('Mean Squarred Error', mean_squared_error(lr_pred, np.exp(Y_valid_set_final), squared=False))\n",
    "print('Mean Absolute Error', mean_absolute_error(lr_pred, np.exp(Y_valid_set_final)))\n",
    "print('R2 Score', r2_score(lr_pred, np.exp(Y_valid_set_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42470cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final_model = Lasso().fit(X_train_set_final, Y_train_set_final)\n",
    "\n",
    "lr_pred = np.exp(lr_final_model.predict(X_valid_set_final))\n",
    "print('Mean Squarred Error', mean_squared_error(lr_pred, np.exp(Y_valid_set_final), squared=False))\n",
    "print('Mean Absolute Error', mean_absolute_error(lr_pred, np.exp(Y_valid_set_final)))\n",
    "print('R2 Score', r2_score(lr_pred, np.exp(Y_valid_set_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db73f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final_model = Ridge().fit(X_train_set_final, Y_train_set_final)\n",
    "\n",
    "lr_pred = np.exp(lr_final_model.predict(X_valid_set_final))\n",
    "print('Mean Squarred Error', mean_squared_error(lr_pred, np.exp(Y_valid_set_final), squared=False))\n",
    "print('Mean Absolute Error', mean_absolute_error(lr_pred, np.exp(Y_valid_set_final)))\n",
    "print('R2 Score', r2_score(lr_pred, np.exp(Y_valid_set_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98179a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final_model = RANSACRegressor(random_state=42).fit(X_train_set_final, Y_train_set_final)\n",
    "\n",
    "lr_pred = np.exp(lr_final_model.predict(X_valid_set_final))\n",
    "print('Mean Squarred Error', mean_squared_error(lr_pred, np.exp(Y_valid_set_final), squared=False))\n",
    "print('Mean Absolute Error', mean_absolute_error(lr_pred, np.exp(Y_valid_set_final)))\n",
    "print('R2 Score', r2_score(lr_pred, np.exp(Y_valid_set_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1977be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final_model = TheilSenRegressor(random_state=42).fit(X_train_set_final, Y_train_set_final)\n",
    "\n",
    "lr_pred = np.exp(lr_final_model.predict(X_valid_set_final))\n",
    "print('Mean Squarred Error', mean_squared_error(lr_pred, np.exp(Y_valid_set_final), squared=False))\n",
    "print('Mean Absolute Error', mean_absolute_error(lr_pred, np.exp(Y_valid_set_final)))\n",
    "print('R2 Score', r2_score(lr_pred, np.exp(Y_valid_set_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems th best one is the Linear one, but it is more propense to outliers. Will go for Hubber\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lr_scores = cross_val_score(\n",
    "    RANSACRegressor(random_state=42).fit(X_train_set_final, Y_train_set_final),\n",
    "    X_train_set_final, Y_train_set_final,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not very significant, as we should be in exponential of the predictions\n",
    "np.sqrt(-lr_scores.mean()), np.sqrt(lr_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd3539",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ff590",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_final_model = DecisionTreeRegressor(random_state=42).fit(X_train_set_final, Y_train_set_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = np.exp(dt_final_model.predict(X_valid_set_final))\n",
    "print('Mean Squarred Error', mean_squared_error(lr_pred, np.exp(Y_valid_set_final), squared=False))\n",
    "print('Mean Absolute Error', mean_absolute_error(lr_pred, np.exp(Y_valid_set_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can see a huge overfitting\n",
    "print(mean_squared_error(np.exp(dt_final_model.predict(X_train_set_final)), np.exp(Y_train_set_final), squared=False))\n",
    "print(mean_squared_error(np.exp(dt_final_model.predict(X_valid_set_final)), np.exp(Y_valid_set_final), squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can try search to not overfit\n",
    "space_dt = { \n",
    "    'max_depth': [12, 15, 20, 25],\n",
    "    'min_samples_leaf': [16, 20],\n",
    "    'min_samples_split': [30, 40],\n",
    "    'max_features': [8, 10, 12],\n",
    "}\n",
    "\n",
    "# Want to be very precise\n",
    "scoring = ['neg_mean_squared_error']\n",
    "search_model_dt = DecisionTreeRegressor()\n",
    "\n",
    "grid_search_final_dt = GridSearchCV(\n",
    "    estimator=search_model_dt,\n",
    "    param_grid=space_dt, \n",
    "    scoring=scoring, \n",
    "    n_jobs=-1, \n",
    "    refit=scoring[0],\n",
    "    cv=3, \n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "grid_search_final_dt.fit(X_train_set_final, Y_train_set_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_final_dt.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5664cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(np.exp(grid_search_final_dt.predict(X_train_set_final)), np.exp(Y_train_set_final), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307055a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(np.exp(grid_search_final_dt.predict(X_valid_set_final)), np.exp(Y_valid_set_final), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac064e0",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can try search to not overfit\n",
    "space_rf = { \n",
    "    'n_estimators': [30, 60, 80, 100],\n",
    "    'max_depth': [3, 4, 6, 8, 10, 12],\n",
    "    'min_samples_leaf': [18, 20],\n",
    "    'min_samples_split': [30, 40],\n",
    "    'max_features': [4, 6, 8, 10, 12],\n",
    "}\n",
    "\n",
    "# Want to be very precise\n",
    "scoring = ['neg_mean_squared_error']\n",
    "search_model_rf = RandomForestRegressor()\n",
    "\n",
    "grid_search_final_rf = GridSearchCV(\n",
    "    estimator=search_model_rf,\n",
    "    param_grid=space_rf, \n",
    "    scoring=scoring, \n",
    "    n_jobs=-1, \n",
    "    refit=scoring[0],\n",
    "    cv=3, \n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "grid_search_final_rf.fit(X_train_set_final, Y_train_set_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91233d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_final_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43291a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_final_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1acbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "{**grid_search_final_rf.best_params_, 'max_depth': 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93588b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = {**grid_search_final_rf.best_params_, 'max_depth': 14, 'n_estimators': 80, 'max_features': 14}\n",
    "best_model = RandomForestRegressor(**{key: int(value) for key, value in best_hp.items()}).fit(X_train_set_final, Y_train_set_final)\n",
    "\n",
    "print(mean_squared_error(np.exp(best_model.predict(X_train_set_final)), np.exp(Y_train_set_final), squared=False))\n",
    "print(mean_squared_error(np.exp(best_model.predict(X_valid_set_final)), np.exp(Y_valid_set_final), squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(np.exp(grid_search_final_rf.predict(X_train_set_final)), np.exp(Y_train_set_final), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c973d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(np.exp(grid_search_final_rf.predict(X_valid_set_final)), np.exp(Y_valid_set_final), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d0c2a",
   "metadata": {},
   "source": [
    "Best so far: 331276.7979632767, and in train 312422.0375526496.\n",
    "\n",
    "{'max_depth': 12,\n",
    " 'max_features': 10,\n",
    " 'min_samples_leaf': 18,\n",
    " 'min_samples_split': 30,\n",
    " 'n_estimators': 60}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e15ccc",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16934c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efbc1f",
   "metadata": {},
   "source": [
    "First, we will do feature selection to reduce amount of variance that the model can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train with all variables, then feature selection, and try again\n",
    "xgb_final_model = xgb.XGBRegressor().fit(X_train_set_final, Y_train_set_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will use all dataset, to then create feature importances (we do not want now high correlated variables)\n",
    "xgb_scaler = StandardScaler()\n",
    "\n",
    "Y_train_set_xgb = train_set_extended.Price_log\n",
    "X_train_set_xgb = train_set_extended.drop(['Price_log', 'Price'], axis=1)\n",
    "\n",
    "X_train_set_xgb = xgb_scaler.fit_transform(X_train_set_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb70991",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final_model.fit(X_train_set_xgb, Y_train_set_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 15))\n",
    "\n",
    "xgb.plot_importance(xgb_final_model, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_xgb = train_set_extended.drop(['Price_log', 'Price'], axis=1).columns.values\n",
    "feature_names_xgb = feature_names_xgb[xgb_final_model.feature_importances_.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_xgb = feature_names_xgb[:22]\n",
    "\n",
    "X_train_set_xgb = train_set_extended[feature_names_xgb]\n",
    "X_train_set_xgb = xgb_scaler.fit_transform(X_train_set_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9854fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_grid = { \n",
    "    # Percentage of columns to be randomly samples for each tree.\n",
    "    \"colsample_bytree\": [ 0.3, 0.5 , 0.8 ],\n",
    "    # reg_alpha provides l1 regularization to the weight, higher values result in more conservative models\n",
    "    \"reg_alpha\": [0, 0.5, 1, 5, 10, 20],\n",
    "    # reg_lambda provides l2 regularization to the weight, higher values result in more conservative models\n",
    "    \"reg_lambda\": [0, 0.5, 1, 5],\n",
    "    'max_depth': [3, 6, 10, 15, 20],\n",
    "    'n_estimators': [50, 100, 300, 500]\n",
    "}\n",
    "\n",
    "# Want to be very precise\n",
    "scoring = ['neg_mean_squared_error']\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    estimator=xgb.XGBRegressor(),\n",
    "    param_grid=param_grid, \n",
    "    scoring=scoring, \n",
    "    refit=scoring[0], # in recall before\n",
    "    n_jobs=-1, \n",
    "    cv=3, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search_xgb = grid_search_xgb.fit(X_train_set_xgb, Y_train_set_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a592b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_xgb = xgb.XGBRegressor(**grid_search_xgb.best_params_).fit(X_train_set_xgb, Y_train_set_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4227c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(np.exp(best_model_xgb.predict(X_train_set_xgb)), np.exp(Y_train_set_xgb), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_valid_set_xgb = valid_set_extended.Price_log\n",
    "X_valid_set_xgb = valid_set_extended[feature_names_xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_set_xgb = xgb_scaler.transform(X_valid_set_xgb)\n",
    "\n",
    "mean_squared_error(np.exp(best_model_xgb.predict(X_valid_set_xgb)), np.exp(Y_valid_set_xgb), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5f5fe",
   "metadata": {},
   "source": [
    "- 198566 in train set score, 297286 in valid set score. 18 features\n",
    "- 223900 in train set score, 296663 in valid set score. 22 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_set_xgb = test_set_extended[feature_names_xgb]\n",
    "X_test_set_xgb = xgb_scaler.transform(X_test_set_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c961f",
   "metadata": {},
   "source": [
    "## Model Creation: Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98196e",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "We have seen Decision Trees & Random Forests tend to overfit a lot. So we will go to the simple solution we had: Linear Regression (Huber implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6285dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_final_model = Ridge().fit(X_train_set_final, Y_train_set_final)\n",
    "submission = pd.DataFrame(np.exp(best_model_xgb.predict(X_test_set_xgb)))\n",
    "submission = submission.reset_index()\n",
    "submission.columns = ['index', 'Price']\n",
    "submission.to_csv('Submissions.csv', index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
